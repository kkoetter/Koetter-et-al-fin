{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import flammkuchen as fl\n",
    "\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from megabouts_helper import labels_cat, color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "from cycler import cycler\n",
    "\n",
    "from megabouts.tracking_data import TrackingConfig, FullTrackingData\n",
    "from megabouts.pipeline import FullTrackingPipeline\n",
    "from megabouts.utils import (\n",
    "    bouts_category_name,\n",
    "    bouts_category_name_short,\n",
    "    bouts_category_color,\n",
    "    cmp_bouts,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_angle_between_vect_tail(v1, v2):\n",
    "    dot = np.einsum('ijk,ijk->ij',[v1,v1,v2],[v2,v1,v2])\n",
    "    cos_= dot[0,:]\n",
    "    sin_= np.cross(v1,v2)\n",
    "    angle_= np.arctan2(sin_,cos_)\n",
    "    return angle_\n",
    "\n",
    "\n",
    "def compute_angle_between_vect(u,v):\n",
    "    u = u/np.linalg.norm(u)\n",
    "    v = v/np.linalg.norm(v)\n",
    "    return np.arctan2(u[0]*v[1]-u[1]*v[0],u[0]*v[0]+u[1]*v[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_body_angle(head_x, head_y, body_x, body_y):\n",
    "    \"\"\"\n",
    "    Computes the angle between two points in 2D space.\n",
    "    \n",
    "    Parameters:\n",
    "    head_x, head_y: Coordinates of the first point (head).\n",
    "    body_x, body_y: Coordinates of the second point (body).\n",
    "    \n",
    "    Returns:\n",
    "    angles_radians: The angle in radians.\n",
    "    angles_degrees: The angle in degrees.\n",
    "    \"\"\"\n",
    "    # Calculate the differences in the x and y coordinates\n",
    "    delta_x = body_x - head_x\n",
    "    delta_y = body_y - head_y\n",
    "\n",
    "    # Calculate the angle using numpy's arctan2\n",
    "    angles_radians = np.arctan2(delta_y, delta_x)\n",
    "\n",
    "    # Convert the angle from radians to degrees\n",
    "    angles_degrees = np.degrees(angles_radians)\n",
    "\n",
    "    return angles_radians, angles_degrees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "### compute fin and body angles\n",
    "def fin_preprocess(df, mid_headx, mid_heady, body_x, body_y):\n",
    "    ##Fin angle computatright\n",
    "    #Fin angle computatright\n",
    "    right_fin_tip_x =  df['right_fin_tip'].values[:, 0].astype('float')\n",
    "    right_fin_tip_y =  df['right_fin_tip'].values[:, 1].astype('float')\n",
    "    right_fin_base_x =  df['right_fin_base'].values[:, 0].astype('float')\n",
    "    right_fin_base_y =  df['right_fin_base'].values[:, 1].astype('float')\n",
    "\n",
    "    left_fin_tip_x =  df['left_fin_tip'].values[:, 0].astype('float')\n",
    "    left_fin_tip_y =  df['left_fin_tip'].values[:, 1].astype('float')\n",
    "    left_fin_base_x =   df['left_fin_base'].values[:, 0].astype('float')\n",
    "    left_fin_base_y =   df['left_fin_base'].values[:, 1].astype('float')\n",
    "\n",
    "    # lets make all the vectors\n",
    "    a = left_fin_base_x-left_fin_tip_x\n",
    "    b = left_fin_base_y-left_fin_tip_y\n",
    "    left_fin_vect = np.array([b,-a])\n",
    "\n",
    "    a = right_fin_base_x-right_fin_tip_x \n",
    "    b = right_fin_base_y-right_fin_tip_y\n",
    "    right_fin_vect = np.array([-b,a])\n",
    "\n",
    "    body_vect = np.vstack((mid_headx -body_x , mid_heady - body_y)) \n",
    "\n",
    "    ## Compute angles between vectors\n",
    "    left_fin_angle =  compute_angle_between_vect(left_fin_vect, body_vect)\n",
    "    right_fin_angle =  compute_angle_between_vect(right_fin_vect, body_vect)\n",
    "\n",
    "    #nan movement artifacts\n",
    "    left_fin_angle = left_fin_angle - left_fin_angle[0]\n",
    "    right_fin_angle = right_fin_angle - right_fin_angle[0]\n",
    "    left_fin_angle[abs(np.diff(left_fin_angle, prepend=[0])) >= 2] = 0 #np.nan #np.pi\n",
    "    right_fin_angle[abs(np.diff(right_fin_angle, prepend=[0])) >= 2] = 0 #np.nan #np.pi\n",
    "    \n",
    "    return left_fin_vect, right_fin_vect, left_fin_angle, right_fin_angle\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Bouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_path = Path(r'\\\\portulab.synology.me\\data\\Kata\\testdata\\Raw_Data')\n",
    "\n",
    "# fish_paths = list(master_path.glob('*f[0-9]*'))\n",
    "# fish_paths, len(fish_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analysed for paper\n",
    "\n",
    "# master_path = Path(r\"\\\\portulab.synology.me\\data\\Kata\\Data\\230307_visstim_2D\") #rectangular arena # start from fish 1\n",
    "# master_path = Path(r\"\\\\portulab.synology.me\\data\\Kata\\Data\\22042024_visstim_2D_round\")\n",
    "master_path = Path(r\"\\\\portulab.synology.me\\data\\Kata\\Data\\22042024_visstim_2D_2\") #rectangular arena\n",
    "# master_path = Path(r\"\\\\portulab.synology.me\\data\\Kata\\Data\\13052024_visstim_2D_round\")\n",
    "# master_path = Path(r\"\\\\portulab.synology.me\\data\\Kata\\Data\\14052024_visstim_2D_round\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([WindowsPath('//portulab.synology.me/data/Kata/Data/22042024_visstim_2D_2/240422_f0'),\n",
       "  WindowsPath('//portulab.synology.me/data/Kata/Data/22042024_visstim_2D_2/240422_f0_1'),\n",
       "  WindowsPath('//portulab.synology.me/data/Kata/Data/22042024_visstim_2D_2/240422_f0_2'),\n",
       "  WindowsPath('//portulab.synology.me/data/Kata/Data/22042024_visstim_2D_2/240422_f1_1'),\n",
       "  WindowsPath('//portulab.synology.me/data/Kata/Data/22042024_visstim_2D_2/240422_f2'),\n",
       "  WindowsPath('//portulab.synology.me/data/Kata/Data/22042024_visstim_2D_2/240422_f3'),\n",
       "  WindowsPath('//portulab.synology.me/data/Kata/Data/22042024_visstim_2D_2/240422_f4'),\n",
       "  WindowsPath('//portulab.synology.me/data/Kata/Data/22042024_visstim_2D_2/240422_f5'),\n",
       "  WindowsPath('//portulab.synology.me/data/Kata/Data/22042024_visstim_2D_2/240422_f6'),\n",
       "  WindowsPath('//portulab.synology.me/data/Kata/Data/22042024_visstim_2D_2/240422_f7'),\n",
       "  WindowsPath('//portulab.synology.me/data/Kata/Data/22042024_visstim_2D_2/240422_f8'),\n",
       "  WindowsPath('//portulab.synology.me/data/Kata/Data/22042024_visstim_2D_2/240422_f9')],\n",
       " 12)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fish_paths = list(master_path.glob('*f[0-9]*'))\n",
    "fish_paths, len(fish_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('22042024_visstim_2D_2', '240422_f1_1')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fish= 3\n",
    "fish_path = fish_paths[fish]\n",
    "fish_id =  fish_paths[fish].name#[:-13]\n",
    "exp_name = Path(fish_paths[fish]).parts[-2]\n",
    "# exp_name = 'testfish'\n",
    "exp_name, fish_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_path = Path(r'\\\\portulab.synology.me\\data\\Kata\\testdata\\Processed_Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analysed for paper\n",
    "\n",
    "# out_path = Path(r\"\\\\portulab.synology.me\\data\\Kata\\Processed_Data\\230307_visstim_2D_\")\n",
    "# out_path = Path(r\"\\\\portulab.synology.me\\data\\Kata\\Processed_Data\\22042024_visstim_2D_round_\")\n",
    "out_path = Path(r\"\\\\portulab.synology.me\\data\\Kata\\Processed_Data\\22042024_visstim_2D_2_\")\n",
    "# out_path = Path(r\"\\\\portulab.synology.me\\data\\Kata\\Processed_Data\\13052024_visstim_2D_round_\")\n",
    "# out_path = Path(r\"\\\\portulab.synology.me\\data\\Kata\\Processed_Data\\14052024_visstim_2D_round_\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load DLC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps=200\n",
    "mm_per_unit = 1/70\n",
    "N_seg = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/12 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fish 240422_f0\n",
      "9.354083333333334 minutes at 200 fps\n",
      "working on 112249 frames\n",
      "(112249,) (112249,) (10, 112249) (10, 112249)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\miniconda3\\envs\\megabouts\\Lib\\site-packages\\megabouts\\classification\\classification.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(transformer_weights_path,map_location=torch.device(self.device)))\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_26028\\4129861763.py:62: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  'cluster_n_vector': np.asarray(ethogram.df[(\"bout\", \"cat\")].values),\n",
      "  8%|██████▉                                                                            | 1/12 [00:36<06:42, 36.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fish 240422_f0_1\n",
      "9.003 minutes at 200 fps\n",
      "working on 108036 frames\n",
      "(108036,) (108036,) (10, 108036) (10, 108036)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\miniconda3\\envs\\megabouts\\Lib\\site-packages\\megabouts\\classification\\classification.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(transformer_weights_path,map_location=torch.device(self.device)))\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_26028\\4129861763.py:62: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  'cluster_n_vector': np.asarray(ethogram.df[(\"bout\", \"cat\")].values),\n",
      " 17%|█████████████▊                                                                     | 2/12 [01:09<05:46, 34.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fish 240422_f0_2\n",
      "8.945666666666666 minutes at 200 fps\n",
      "working on 107348 frames\n",
      "(107348,) (107348,) (10, 107348) (10, 107348)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\miniconda3\\envs\\megabouts\\Lib\\site-packages\\megabouts\\classification\\classification.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(transformer_weights_path,map_location=torch.device(self.device)))\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_26028\\4129861763.py:62: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  'cluster_n_vector': np.asarray(ethogram.df[(\"bout\", \"cat\")].values),\n",
      " 25%|████████████████████▊                                                              | 3/12 [01:43<05:06, 34.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fish 240422_f1_1\n",
      "8.896666666666667 minutes at 200 fps\n",
      "working on 106760 frames\n",
      "(106760,) (106760,) (10, 106760) (10, 106760)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\miniconda3\\envs\\megabouts\\Lib\\site-packages\\megabouts\\classification\\classification.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(transformer_weights_path,map_location=torch.device(self.device)))\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_26028\\4129861763.py:62: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  'cluster_n_vector': np.asarray(ethogram.df[(\"bout\", \"cat\")].values),\n",
      " 33%|███████████████████████████▋                                                       | 4/12 [02:16<04:30, 33.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fish 240422_f2\n",
      "8.878916666666667 minutes at 200 fps\n",
      "working on 106547 frames\n",
      "(106547,) (106547,) (10, 106547) (10, 106547)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\miniconda3\\envs\\megabouts\\Lib\\site-packages\\megabouts\\classification\\classification.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(transformer_weights_path,map_location=torch.device(self.device)))\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_26028\\4129861763.py:62: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  'cluster_n_vector': np.asarray(ethogram.df[(\"bout\", \"cat\")].values),\n",
      " 42%|██████████████████████████████████▌                                                | 5/12 [02:50<03:55, 33.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fish 240422_f3\n",
      "7.849333333333333 minutes at 200 fps\n",
      "working on 94192 frames\n",
      "(94192,) (94192,) (10, 94192) (10, 94192)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\miniconda3\\envs\\megabouts\\Lib\\site-packages\\megabouts\\classification\\classification.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(transformer_weights_path,map_location=torch.device(self.device)))\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_26028\\4129861763.py:62: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  'cluster_n_vector': np.asarray(ethogram.df[(\"bout\", \"cat\")].values),\n",
      " 50%|█████████████████████████████████████████▌                                         | 6/12 [03:18<03:11, 31.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fish 240422_f4\n",
      "9.144166666666667 minutes at 200 fps\n",
      "working on 109730 frames\n",
      "(109730,) (109730,) (10, 109730) (10, 109730)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\miniconda3\\envs\\megabouts\\Lib\\site-packages\\megabouts\\classification\\classification.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(transformer_weights_path,map_location=torch.device(self.device)))\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_26028\\4129861763.py:62: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  'cluster_n_vector': np.asarray(ethogram.df[(\"bout\", \"cat\")].values),\n",
      " 58%|████████████████████████████████████████████████▍                                  | 7/12 [03:52<02:42, 32.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fish 240422_f5\n",
      "6.742416666666666 minutes at 200 fps\n",
      "working on 80909 frames\n",
      "(80909,) (80909,) (10, 80909) (10, 80909)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\miniconda3\\envs\\megabouts\\Lib\\site-packages\\megabouts\\classification\\classification.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(transformer_weights_path,map_location=torch.device(self.device)))\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_26028\\4129861763.py:62: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  'cluster_n_vector': np.asarray(ethogram.df[(\"bout\", \"cat\")].values),\n",
      " 67%|███████████████████████████████████████████████████████▎                           | 8/12 [04:17<02:00, 30.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fish 240422_f6\n",
      "7.639666666666667 minutes at 200 fps\n",
      "working on 91676 frames\n",
      "(91676,) (91676,) (10, 91676) (10, 91676)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\miniconda3\\envs\\megabouts\\Lib\\site-packages\\megabouts\\classification\\classification.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(transformer_weights_path,map_location=torch.device(self.device)))\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_26028\\4129861763.py:62: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  'cluster_n_vector': np.asarray(ethogram.df[(\"bout\", \"cat\")].values),\n",
      " 75%|██████████████████████████████████████████████████████████████▎                    | 9/12 [04:45<01:28, 29.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fish 240422_f7\n",
      "8.353333333333333 minutes at 200 fps\n",
      "working on 100240 frames\n",
      "(100240,) (100240,) (10, 100240) (10, 100240)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\miniconda3\\envs\\megabouts\\Lib\\site-packages\\megabouts\\classification\\classification.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(transformer_weights_path,map_location=torch.device(self.device)))\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_26028\\4129861763.py:62: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  'cluster_n_vector': np.asarray(ethogram.df[(\"bout\", \"cat\")].values),\n",
      " 83%|████████████████████████████████████████████████████████████████████▎             | 10/12 [05:16<01:00, 30.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fish 240422_f8\n",
      "8.89475 minutes at 200 fps\n",
      "working on 106737 frames\n",
      "(106737,) (106737,) (10, 106737) (10, 106737)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\miniconda3\\envs\\megabouts\\Lib\\site-packages\\megabouts\\classification\\classification.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load(transformer_weights_path,map_location=torch.device(self.device)))\n",
      " 83%|████████████████████████████████████████████████████████████████████▎             | 10/12 [05:48<01:09, 34.81s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (10,0) into shape (10,40)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m pipeline\u001b[38;5;241m.\u001b[39msegmentation_cfg\u001b[38;5;241m.\u001b[39mthreshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m\n\u001b[0;32m     54\u001b[0m pipeline\u001b[38;5;241m.\u001b[39mtail_preprocessing_cfg\u001b[38;5;241m.\u001b[39mtail_speed_filter_ms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m---> 56\u001b[0m ethogram, bouts, segments, tail, traj \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtracking_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m### Save object\u001b[39;00m\n\u001b[0;32m     59\u001b[0m megabouts_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m({\n\u001b[0;32m     60\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msegments_on\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39masarray(segments\u001b[38;5;241m.\u001b[39monset),\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msegments_off\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39masarray(segments\u001b[38;5;241m.\u001b[39moffset),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     79\u001b[0m \n\u001b[0;32m     80\u001b[0m })\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\megabouts\\Lib\\site-packages\\megabouts\\pipeline\\freely_swimming_pipeline.py:231\u001b[0m, in \u001b[0;36mFullTrackingPipeline.run\u001b[1;34m(self, tracking_data)\u001b[0m\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msegmentation_cfg should be an instance of TailSegmentationConfig or TrajSegmentationConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;66;03m#self.logger.info(\"Classification...\")\u001b[39;00m\n\u001b[1;32m--> 231\u001b[0m bouts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassify_bouts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtail\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtraj\u001b[49m\u001b[43m,\u001b[49m\u001b[43msegments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m ethogram \u001b[38;5;241m=\u001b[39m EthogramFullTracking(segments,bouts,tail,traj)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ethogram,bouts,segments,tail,traj\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\megabouts\\Lib\\site-packages\\megabouts\\pipeline\\freely_swimming_pipeline.py:201\u001b[0m, in \u001b[0;36mFullTrackingPipeline.classify_bouts\u001b[1;34m(self, tail, traj, segments)\u001b[0m\n\u001b[0;32m    196\u001b[0m classif_results \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mrun_classification(tail_array\u001b[38;5;241m=\u001b[39mtail_array,\n\u001b[0;32m    197\u001b[0m                                                 traj_array\u001b[38;5;241m=\u001b[39mtraj_array)\n\u001b[0;32m    198\u001b[0m segments\u001b[38;5;241m.\u001b[39mset_HB1(classif_results[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst_half_beat\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 201\u001b[0m tail_array \u001b[38;5;241m=\u001b[39m  \u001b[43msegments\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_tail_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtail_angle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtail\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mangle_smooth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m                                        \u001b[49m\u001b[43malign_to_onset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m traj_array \u001b[38;5;241m=\u001b[39m  segments\u001b[38;5;241m.\u001b[39mextract_traj_array(head_x\u001b[38;5;241m=\u001b[39mtraj\u001b[38;5;241m.\u001b[39mx_smooth,\n\u001b[0;32m    205\u001b[0m                                         head_y\u001b[38;5;241m=\u001b[39mtraj\u001b[38;5;241m.\u001b[39my_smooth,\n\u001b[0;32m    206\u001b[0m                                         head_angle\u001b[38;5;241m=\u001b[39mtraj\u001b[38;5;241m.\u001b[39myaw_smooth,\n\u001b[0;32m    207\u001b[0m                                         align_to_onset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    209\u001b[0m bouts \u001b[38;5;241m=\u001b[39m TailBouts(segments\u001b[38;5;241m=\u001b[39msegments,\n\u001b[0;32m    210\u001b[0m                   classif_results\u001b[38;5;241m=\u001b[39mclassif_results,\n\u001b[0;32m    211\u001b[0m                   tail_array\u001b[38;5;241m=\u001b[39mtail_array,\n\u001b[0;32m    212\u001b[0m                   traj_array\u001b[38;5;241m=\u001b[39mtraj_array)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\megabouts\\Lib\\site-packages\\megabouts\\segmentation\\segmentation.py:39\u001b[0m, in \u001b[0;36mSegmentationResult.extract_tail_array\u001b[1;34m(self, tail_angle, align_to_onset)\u001b[0m\n\u001b[0;32m     37\u001b[0m     id_ed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(id_ed,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     38\u001b[0m     dur \u001b[38;5;241m=\u001b[39m id_ed\u001b[38;5;241m-\u001b[39mid_st\n\u001b[1;32m---> 39\u001b[0m     \u001b[43mtail_array\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mdur\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m tail_angle[id_st:id_ed, :]\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tail_array\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (10,0) into shape (10,40)"
     ]
    }
   ],
   "source": [
    "\n",
    "# for ind, fish_path in enumerate(tqdm(fish_paths[1:])):\n",
    "for ind, fish_path in enumerate(tqdm(fish_paths)):\n",
    "    fish_id =  fish_path.name\n",
    "    print ('Working on fish {}'.format(fish_id))\n",
    "    df = pd.read_csv(out_path/ '{}_DLC_mod.csv'.format(fish_id), header=[0, 1])\n",
    "    print(f'{df.shape[0]/(fps*60)} minutes at {fps} fps')\n",
    "    print('working on {} frames'.format(df.shape[0]))\n",
    "    \n",
    "    #Extract angles\n",
    "    body_x = df.body.values[:, 0].astype('float')\n",
    "    body_y = df.body.values[:, 1].astype('float')\n",
    "    \n",
    "    # Compute head and tail coordinates and convert to mm\n",
    "    tail_x_col = [f'tail_{i}' for i in range(N_seg)]\n",
    "    tail_y_col = [f'tail_{i}' for i in range(N_seg)]\n",
    "    tail_x = np.array([df[x].iloc[:, 0].values.astype('float') for x in tail_x_col]) * mm_per_unit\n",
    "    tail_y = np.array([df[x].iloc[:, 1].values.astype('float') for x in tail_y_col]) * mm_per_unit\n",
    "    head_x = df.mid_head.values[:, 0].astype('float') * mm_per_unit\n",
    "    head_y = df.mid_head.values[:, 1].astype('float') * mm_per_unit\n",
    "\n",
    "    # compute body angle\n",
    "    body_x_ = np.asarray(df.body.values[:, 0].astype('float'))\n",
    "    body_y_ = np.asarray(df.body.values[:, 1].astype('float'))\n",
    "    head_x_ = np.asarray(df.mid_head.values[:, 0].astype('float'))\n",
    "    head_y_ = np.asarray(df.mid_head.values[:, 1].astype('float'))\n",
    "    body_angle, angles_degrees = compute_body_angle(head_x_, head_y_, body_x_, body_y_)\n",
    "\n",
    "    # compute fin angles\n",
    "    left_fin_vect, right_fin_vect, left_fin_angle, right_fin_angle = fin_preprocess(df, head_x_, head_y_, body_x_, body_y_)\n",
    "\n",
    "    # Load eye stuff\n",
    "    eye_angles = fl.load(fish_path/'eye_angles.h5')['eye_angles'] #for the hdf5 way of saving dict needs ['eye_angles']\n",
    "    vergence = fl.load(fish_path/'eye_rot.h5')['eye_rot']\n",
    "    rotation_eye = fl.load(fish_path/'eye_verg.h5')['eye_verg']\n",
    "    eye_coords = fl.load(fish_path/'eye_coords.h5')['eye_coords']\n",
    "    \n",
    "    left_eye_angle = np.rad2deg(eye_angles[:,0])\n",
    "    right_eye_angle = np.rad2deg(eye_angles[:,1])\n",
    "    rotation_eye = np.rad2deg(rotation_eye)\n",
    "\n",
    "\n",
    "    ### MB pipeline\n",
    "    # Load data and set tracking configuration\n",
    "    tracking_cfg = TrackingConfig(fps=fps, tracking=\"full_tracking\")\n",
    "\n",
    "    # Create FullTrackingData object\n",
    "    tracking_data = FullTrackingData.from_keypoints(\n",
    "        head_x=head_x, head_y=head_y, tail_x=tail_x.T, tail_y=tail_y.T)\n",
    "\n",
    "    print (head_x.shape, head_y.shape, tail_x.shape, tail_y.shape)\n",
    "\n",
    "    pipeline = FullTrackingPipeline(tracking_cfg, exclude_CS=True)\n",
    "    pipeline.segmentation_cfg.threshold = 20\n",
    "    pipeline.tail_preprocessing_cfg.tail_speed_filter_ms = 50\n",
    "\n",
    "    ethogram, bouts, segments, tail, traj = pipeline.run(tracking_data)\n",
    "\n",
    "    ### Save object\n",
    "    megabouts_res = dict({\n",
    "    'segments_on': np.asarray(segments.onset),\n",
    "    'segments_off': np.asarray(segments.offset),\n",
    "    'cluster_n_vector': np.asarray(ethogram.df[(\"bout\", \"cat\")].values),\n",
    "    'clusters':  np.asarray(bouts.df.label.category),\n",
    "    'laterality' :np.asarray(bouts.df.label.sign),\n",
    "    'proba' :np.asarray(bouts.df.label.proba),\n",
    "    \n",
    "    'clean_data_tail':np.asarray(ethogram.df[\"tail_angle\"].values),\n",
    "    'body_angle' :np.asarray(body_angle),\n",
    "    'body_angle_rad' :np.asarray(angles_degrees),\n",
    "    'head_angle_mb' :np.asarray(traj.yaw_smooth),\n",
    "    'duration' : np.asarray(bouts.df.location.offset - bouts.df.location.onset),\n",
    "    'bouts_df': bouts.df, \n",
    "    'ethogram_df': ethogram.df,\n",
    "\n",
    "    'fin_angles': np.asarray([left_fin_angle, right_fin_angle]),\n",
    "    'eye_angles': np.asarray([left_eye_angle, right_eye_angle]), \n",
    "    'vergence': np.asarray(vergence),\n",
    "    'rotation': np.asarray(rotation_eye), \n",
    "    \n",
    "    })\n",
    "    fl.save(out_path/'{}_megabouts_res.h5'.format(fish_id), megabouts_res)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "megabouts_res.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(megabouts_res['clusters'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "945ab21136d100d39eb08c79ef7fc552f9de38f223a833a821377820191bf364"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
