{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb1d7f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29d0fe61",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0a3ca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import glob\n",
    "\n",
    "import h5py\n",
    "import flammkuchen as fl\n",
    "import seaborn as sns\n",
    "from matplotlib.cm import ScalarMappable\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f807f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import threshold_otsu\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "def extract_extrema(arr):    \n",
    "    idxs = np.arange(arr.shape[0])\n",
    "    min_idxs = []\n",
    "    max_idxs = []\n",
    "    for i in range(1, arr.shape[0] - 1):\n",
    "        if arr[i - 1] < arr[i] > arr[i + 1]:\n",
    "            max_idxs.append(i)\n",
    "        elif arr[i - 1] > arr[i] < arr[i + 1]:\n",
    "            min_idxs.append(i)\n",
    "    return min_idxs, max_idxs\n",
    "    \n",
    "def moving_average(a, n=2):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "def smooth_trace(trace, wnd=9, poly=2):\n",
    "    return savgol_filter(trace, wnd, poly)  # window size 5, polynomial order 2\n",
    "\n",
    "def find_extrema_and_peaks(data, thr=.1):\n",
    "    extrema_idxs = []\n",
    "    extrema_diffs = []\n",
    "\n",
    "    for trace in data:\n",
    "        # Identify extrema\n",
    "        mins, maxs = extract_extrema(trace)\n",
    "\n",
    "        # Combine, sort, and add indexes for first and last timepoints (needed to compute all needed differentials)\n",
    "        extrema = np.sort(np.concatenate((np.array(mins), np.array(maxs))))\n",
    "        extrema = np.insert(extrema, 0, 0)\n",
    "        extrema = np.append(extrema, trace.shape[-1]-1)\n",
    "\n",
    "        # Normalize trace to [0,1] range and compute average diff (before and after) for each extrema\n",
    "        trace_norm = (trace-trace.min())/(trace.max()-trace.min())\n",
    "        extrema_y = [trace_norm[i] for i in extrema]\n",
    "        extrema_diff = np.abs(np.diff(extrema_y))\n",
    "        avg_extrema_diff = moving_average(extrema_diff)\n",
    "\n",
    "        # Append values\n",
    "        extrema_idxs.append(extrema)\n",
    "        extrema_diffs.append(avg_extrema_diff)\n",
    "\n",
    "    # Find Otsu threshold for extrema inclusion\n",
    "    # thr = threshold_otsu(np.concatenate(extrema_diffs))\n",
    "\n",
    "    # Identify real extrema\n",
    "    real_extrema = [bout > thr for bout in extrema_diffs]\n",
    "\n",
    "    # Recapitulate positions and amplitudes for all extrema\n",
    "    real_extrema_idxs = [extrema_idxs[bout][1:-1][real_extrema[bout]] for bout in range(len(real_extrema))]\n",
    "    real_extrema_diff = [[np.diff(data[bout])[i] for i in real_extrema_idxs[bout]] for bout in range(len(real_extrema))]\n",
    "\n",
    "    peaks_i = [real_extrema_idxs[bout][np.array(real_extrema_diff[bout]) < 0] for bout in range(len(real_extrema))]\n",
    "    peaks_a = [[data[bout, i] for i in peaks_i[bout]] for bout in range(len(real_extrema))]\n",
    "\n",
    "    valleys_i = [real_extrema_idxs[bout][np.array(real_extrema_diff[bout]) > 0] for bout in range(len(real_extrema))]\n",
    "    valleys_a = [[data[bout, i] for i in valleys_i[bout]] for bout in range(len(real_extrema))]\n",
    "    \n",
    "    return peaks_a, peaks_i, valleys_a, valleys_i\n",
    "\n",
    "\n",
    "def extract_peaks_valleys_arrays(data, peaks_a, peaks_i, valleys_a, valleys_i, max_n=9):\n",
    "    \"\"\"\n",
    "    Function to extract and fill arrays for peaks and valleys.\n",
    "\n",
    "    Parameters:\n",
    "    - data: numpy.ndarray, the original data array from which peaks and valleys are derived.\n",
    "    - peaks_a: list of lists, amplitudes of peaks for each trace.\n",
    "    - peaks_i: list of lists, indices of peaks for each trace.\n",
    "    - valleys_a: list of lists, amplitudes of valleys for each trace.\n",
    "    - valleys_i: list of lists, indices of valleys for each trace.\n",
    "    - max_n: int, maximum number of peaks/valleys to store for each trace (default is 9).\n",
    "\n",
    "    Returns:\n",
    "    - peaks_a_array, peaks_i_array, valleys_a_array, valleys_i_array: numpy.ndarrays filled with peak/valley data.\n",
    "    \"\"\"\n",
    "    num_traces = data.shape[0]\n",
    "    \n",
    "    # Initialize arrays with NaN\n",
    "    peaks_a_array = np.full((num_traces, max_n), np.nan)\n",
    "    peaks_i_array = np.full((num_traces, max_n), np.nan)\n",
    "    valleys_a_array = np.full((num_traces, max_n), np.nan)\n",
    "    valleys_i_array = np.full((num_traces, max_n), np.nan)\n",
    "\n",
    "    # Fill arrays with peak and valley data\n",
    "    for i in range(num_traces):\n",
    "        peak_count = min(len(peaks_a[i]), max_n)\n",
    "        valley_count = min(len(valleys_a[i]), max_n)\n",
    "        \n",
    "        peaks_a_array[i, :peak_count] = peaks_a[i][:peak_count]\n",
    "        peaks_i_array[i, :peak_count] = peaks_i[i][:peak_count]\n",
    "        valleys_a_array[i, :valley_count] = valleys_a[i][:valley_count]\n",
    "        valleys_i_array[i, :valley_count] = valleys_i[i][:valley_count]\n",
    "    \n",
    "    return peaks_a_array, peaks_i_array, valleys_a_array, valleys_i_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "886057f9-b05f-41ac-b39c-d149fd0b74e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_feature_array(feature_vector_array):\n",
    "    \"\"\"\n",
    "    Reshape the feature vector array into a specified shape and extract sub-arrays.\n",
    "\n",
    "    Parameters:\n",
    "    - feature_vector_array: numpy.ndarray, the array to be reshaped.\n",
    "\n",
    "    Returns:\n",
    "    - reshaped_array: numpy.ndarray, the reshaped array.\n",
    "    - peaks_a_array, peaks_i_array, valleys_a_array, valleys_i_array: separate sub-arrays.\n",
    "    \"\"\"\n",
    "    max_n = int(feature_vector_array.shape[1] / 4)\n",
    "\n",
    "    # Reshape the array\n",
    "    reshaped_array = feature_vector_array.reshape(feature_vector_array.shape[0], 4, max_n)\n",
    "\n",
    "    # Extract sub-arrays\n",
    "    peaks_a_array = reshaped_array[:, 0, :]\n",
    "    peaks_i_array = reshaped_array[:, 1, :]\n",
    "    valleys_a_array = reshaped_array[:, 2, :]\n",
    "    valleys_i_array = reshaped_array[:, 3, :]\n",
    "\n",
    "    print(f\"Reshaped array shape: {reshaped_array.shape}\")\n",
    "\n",
    "    return reshaped_array, peaks_a_array, peaks_i_array, valleys_a_array, valleys_i_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71bb6357",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"white\", palette=\"pastel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017dcd0d",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b612b342-011e-483f-9712-4e3d3ab11847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# master_path = Path(r'\\\\portulab.synology.me\\data\\Kata\\testdata\\Raw_Data')\n",
    "\n",
    "# fish_paths = list(master_path.glob('*f[0-9]*'))\n",
    "# fish_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2b66c52a-712f-407f-b2ae-7267efd8bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analysed for paper\n",
    "\n",
    "# master_path = Path(r\"\\\\portulab.synology.me\\data\\Kata\\Data\\230307_visstim_2D\") #rectangular arena # start from fish 1\n",
    "# master_path = Path(r\"\\\\portulab.synology.me\\data\\Kata\\Data\\22042024_visstim_2D_round\")\n",
    "# master_path = Path(r\"\\\\portulab.synology.me\\data\\Kata\\Data\\22042024_visstim_2D_2\") #rectangular arena\n",
    "# master_path = Path(r\"\\\\portulab.synology.me\\data\\Kata\\Data\\13052024_visstim_2D_round\")\n",
    "master_path = Path(r\"\\\\portulab.synology.me\\data\\Kata\\Data\\14052024_visstim_2D_round\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e8c37195-e219-455c-8b77-d34923905716",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([WindowsPath('//portulab.synology.me/data/Kata/Data/14052024_visstim_2D_round/240514_f0'),\n",
       "  WindowsPath('//portulab.synology.me/data/Kata/Data/14052024_visstim_2D_round/240514_f1'),\n",
       "  WindowsPath('//portulab.synology.me/data/Kata/Data/14052024_visstim_2D_round/240514_f2'),\n",
       "  WindowsPath('//portulab.synology.me/data/Kata/Data/14052024_visstim_2D_round/240514_f3'),\n",
       "  WindowsPath('//portulab.synology.me/data/Kata/Data/14052024_visstim_2D_round/240514_f4'),\n",
       "  WindowsPath('//portulab.synology.me/data/Kata/Data/14052024_visstim_2D_round/240514_f5'),\n",
       "  WindowsPath('//portulab.synology.me/data/Kata/Data/14052024_visstim_2D_round/240514_f6'),\n",
       "  WindowsPath('//portulab.synology.me/data/Kata/Data/14052024_visstim_2D_round/240514_f7'),\n",
       "  WindowsPath('//portulab.synology.me/data/Kata/Data/14052024_visstim_2D_round/240514_f8'),\n",
       "  WindowsPath('//portulab.synology.me/data/Kata/Data/14052024_visstim_2D_round/240514_f9')],\n",
       " 10)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fish_paths = list(master_path.glob('*f[0-9]*'))\n",
    "fish_paths, len(fish_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f7264ce2-9f2a-4e5c-9eef-af47a00b011a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('240514_f0', '14052024_visstim_2D_round')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fish= 0\n",
    "fish_id =  fish_paths[fish].name\n",
    "exp_name = Path(fish_paths[fish]).parts[-2]\n",
    "# exp_name = 'testfish'\n",
    "fish_id, exp_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c0d64982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# out_path = Path(r'\\\\portulab.synology.me\\data\\Kata\\testdata\\Processed_Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2abd139b-02ed-4bb8-aaab-24186f5a3dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analysed for paper\n",
    "\n",
    "# out_path = Path(r\"\\\\portulab.synology.me\\data\\Kata\\Processed_Data\\230307_visstim_2D_\")\n",
    "# out_path = Path(r\"\\\\portulab.synology.me\\data\\Kata\\Processed_Data\\22042024_visstim_2D_round_\")\n",
    "# out_path = Path(r\"\\\\portulab.synology.me\\data\\Kata\\Processed_Data\\22042024_visstim_2D_2_\")\n",
    "# out_path = Path(r\"\\\\portulab.synology.me\\data\\Kata\\Processed_Data\\13052024_visstim_2D_round_\")\n",
    "out_path = Path(r\"\\\\portulab.synology.me\\data\\Kata\\Processed_Data\\14052024_visstim_2D_round_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0cf2c50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data_path = out_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82a0a55-fc49-4cbb-a890-aff8ae6be75d",
   "metadata": {},
   "source": [
    "## Load traces and make tensor in shape (trials, tails, left fin, right fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "23dec7b0-27b0-4b83-8757-4ae066ec4546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fish 240514_f0\n",
      "(192, 3, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████▌                                                                  | 2/10 [00:00<00:02,  3.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fish 240514_f1\n",
      "(41, 3, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|████████████████████████▉                                                          | 3/10 [00:00<00:01,  4.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fish 240514_f2\n",
      "(40, 3, 50)\n",
      "Working on fish 240514_f3\n",
      "(147, 3, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████▏                                                 | 4/10 [00:01<00:01,  3.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fish 240514_f4\n",
      "(272, 3, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [00:01<00:01,  3.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fish 240514_f5\n",
      "(175, 3, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [00:01<00:01,  3.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fish 240514_f6\n",
      "(277, 3, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [00:02<00:00,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fish 240514_f7\n",
      "(279, 3, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [00:02<00:00,  2.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fish 240514_f8\n",
      "(577, 3, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [00:03<00:00,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on fish 240514_f9\n",
      "(509, 3, 50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  2.76it/s]\n"
     ]
    }
   ],
   "source": [
    "for ind, fish_path in enumerate(tqdm(fish_paths)):\n",
    "    fish_id =  fish_path.name\n",
    "    print ('Working on fish {}'.format(fish_id))\n",
    "\n",
    "    data = fl.load(out_path / '{}_tensor.h5'.format(fish_id))\n",
    "    print(data.shape)\n",
    "    ## specify 0 for tail, 1 for l fin and 2 for right fin\n",
    "    ## because we dont want to confuse peaks and vallyes later -> left invert l fin\n",
    "    data_tail = data[:,0,:]\n",
    "    data_l_fin = data[:,1,:]*-1\n",
    "    data_r_fin = data[:,2,:]\n",
    "\n",
    "    datas = [data_tail, data_l_fin, data_r_fin]\n",
    "    feat_array_lists = []\n",
    "    \n",
    "    for data_ in datas:\n",
    "        # data = np.apply_along_axis(smooth_trace, 1, data_)\n",
    "        peaks_a, peaks_i, valleys_a, valleys_i = find_extrema_and_peaks(data_, thr=.2)\n",
    "    \n",
    "        peaks_a_array, peaks_i_array, valleys_a_array, valleys_i_array = extract_peaks_valleys_arrays(data_, peaks_a, peaks_i, valleys_a, valleys_i, max_n=9)\n",
    "        # concatenate feature array\n",
    "        feature_vector_array = np.concatenate([peaks_a_array, peaks_i_array, valleys_a_array, valleys_i_array], axis=1)\n",
    "        feat_array_lists.append(feature_vector_array)\n",
    "        \n",
    "    fl.save(save_data_path/ '{}_t_feature_vector_array.h5'.format(fish_id), feat_array_lists[0])\n",
    "    fl.save(save_data_path/ '{}_l_feature_vector_array.h5'.format(fish_id), feat_array_lists[1])\n",
    "    fl.save(save_data_path/ '{}_r_feature_vector_array.h5'.format(fish_id), feat_array_lists[2])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21122b18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0398b545-4fc4-47ef-94e1-62df7cac8936",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc9b08b-f069-444b-b54e-42c4f80be65c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2d028f-0f5d-45ed-a7b9-1e44677ce950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78602345-8fd0-4835-bd5f-197a7a0150ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc5e0b8-6094-44b8-afcf-3dc353edac4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8074f8e9-f37f-4e98-b9c7-00e61b47f500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b802e7-97b3-4c13-aa6a-91892e46b5c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c9672b-7501-4d65-8226-0d429ed31e3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c067716-e5be-4b64-bd07-fbafa864a20b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
